{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import keras\n",
    "import keras.backend     as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy             as np\n",
    "\n",
    "from keras.models         import Sequential\n",
    "from keras.layers         import Convolution2D\n",
    "from keras.layers         import MaxPooling2D\n",
    "from keras.layers         import Activation\n",
    "from keras.layers         import Dense\n",
    "from keras.layers         import Flatten\n",
    "from keras.layers         import Reshape\n",
    "from keras.layers         import UpSampling2D\n",
    "from keras.optimizers     import Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.datasets       import mnist\n",
    "\n",
    "path_to_fashion_mnist = '../../datasets/fashion-mnist/'\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, path_to_fashion_mnist)\n",
    "from utils import mnist_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anomalies_number = 400\n",
    "encoding_size    = 200\n",
    "batch_size       = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation\n",
    "\n",
    "## Dataset loading\n",
    "\n",
    "First, we load the fashion MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_fashion, _ = mnist_reader.load_mnist(path_to_fashion_mnist + 'data/fashion', kind = 'train')\n",
    "X_test_fashion, _  = mnist_reader.load_mnist(path_to_fashion_mnist + 'data/fashion', kind = 't10k')\n",
    "X_fashion          = np.concatenate((X_train_fashion, X_test_fashion)).reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8928c86710>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4Qcy9X+lnORLhr0RAwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8928d02b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_fashion[35].squeeze(), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the digit MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train_digits, _), (X_test_digits, _) = mnist.load_data()\n",
    "X_digits                                = np.concatenate((X_train_digits, X_test_digits)).reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f892be6df28>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+KWiDL9UlOGXijL8UlGGXyrK8EtF/S8yWjNyU48IrAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8928c96fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_digits[35].squeeze(), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly dataset creation \n",
    "\n",
    "Now, we extract `anomaly_number` images from the fashion MNIST dataset. Then, we create a big dataset with all the MNIST digits and these `anomaly number` images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anomalies = np.random.permutation(X_fashion)[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f892be016a0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPwsgBA54HdVaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f892be11da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(anomalies[39].squeeze(), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((X_digits, anomalies))\n",
    "X = np.random.permutation(X)\n",
    "X = (X - 128.) / X.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.1983508536269749,\n",
       " 1.0000000000000002,\n",
       " -1.624904856366701,\n",
       " 1.6122102871763362)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.mean(), X.std(), X.min(), X.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicator network\n",
    "\n",
    "The intuition that I have on replicator network is that they are autoencoders (a highly specialized compressor and decompressor) trained on the whole dataset. The idea is that the \"usual\" will be handled quite well by our autoencoder in the sense that the structure of the data will be well understood, i.e. the image produced by encoding and then decoding will be very close to the original one. To discover anomalies in our dataset, we will try to look for images that differ a lot from their compressed and decompressed counterparts.\n",
    "Let's first build and train this autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = Sequential([\n",
    "    Convolution2D(64, (3, 3), padding = 'same', input_shape = (28, 28, 1), activation = 'relu'),\n",
    "    Convolution2D(64, (3, 3), padding = 'same', activation = 'relu'),\n",
    "    MaxPooling2D(),\n",
    "    Convolution2D(128, (3, 3), padding = 'same', activation = 'relu'),\n",
    "    Convolution2D(128, (3, 3), padding = 'same', activation = 'relu'),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    Dense(encoding_size, activation = 'tanh'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder = Sequential([\n",
    "    Dense(49, input_shape = (encoding_size,), activation = 'relu'),\n",
    "    Reshape((7, 7, 1)),\n",
    "    Convolution2D(128, (3, 3), padding = 'same', activation = 'relu'),\n",
    "    Convolution2D(128, (3, 3), padding = 'same', activation = 'relu'),\n",
    "    UpSampling2D(),\n",
    "    Convolution2D(64, (3, 3), padding = 'same', activation = 'relu'),\n",
    "    Convolution2D(64, (3, 3), padding = 'same', activation = 'relu'),\n",
    "    UpSampling2D(),\n",
    "    Convolution2D(1, (3, 3), padding = 'same', activation = 'tanh')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder = Sequential([\n",
    "    encoder,\n",
    "    decoder\n",
    "])\n",
    "autoencoder.compile(optimizer = Adam(1e-4), loss = 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit_params = {\n",
    "    'x'               : X,\n",
    "    'y'               : X,\n",
    "    'batch_size'      : batch_size,\n",
    "    'epochs'          : 30,\n",
    "    'validation_split': 0.15\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59840 samples, validate on 10560 samples\n",
      "Epoch 1/5\n",
      "59840/59840 [==============================] - 125s - loss: 0.5204 - val_loss: 0.4184\n",
      "Epoch 2/5\n",
      "25088/59840 [===========>..................] - ETA: 64s - loss: 0.4089"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d6fed67fc4c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/greg/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/greg/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/home/greg/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/greg/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/greg/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/greg/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/greg/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/greg/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/greg/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "autoencoder.fit(**fit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the network is trained, we want to get an idea of the quality of compression on \"normal\" data and on \"anomalies\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "digit_img = np.expand_dims(X_digits[2929], 0)\n",
    "digit_img_ = autoencoder.predict(digit_img)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(digit_img[0].squeeze(), cmap = 'gray')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(digit_img_[0].squeeze(), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the digit images are recovered pretty well, that is a good first step for what we are trying to implement.\n",
    "\n",
    "Now let's do the same thing with images from the fashion dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fashion_img = np.expand_dims(X_fashion[22], 0)\n",
    "fashion_img_ = autoencoder.predict(fashion_img)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(fashion_img[0].squeeze(), cmap = 'gray')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(fashion_img_[0].squeeze(), cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of the recovered image is terrible, which is really good for our usecase. Since these images are very different from the \"normal\" ones, the autoencoder haven't manage to capture their structures. We can hope that anomalous images will have a reconstruction loss much higher than normal ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder.save_weights('../models/autoencoder_%dD.h5' % encoding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimension 1 in both shapes must be equal, but are 200 and 100 for 'Assign_10' (op: 'Assign') with input shapes: [128,200], [128,100].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/home/greg/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    653\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    655\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/greg/anaconda3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/greg/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimension 1 in both shapes must be equal, but are 200 and 100 for 'Assign_10' (op: 'Assign') with input shapes: [128,200], [128,100].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-6aba525630c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# autoencoder.load_weights('../models/autoencoder_%dD.h5' % encoding_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../models/autoencoder_%dD.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/greg/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0mtopology\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             \u001b[0mtopology\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/greg/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m   3093\u001b[0m                              ' elements.')\n\u001b[1;32m   3094\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3095\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/greg/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2186\u001b[0m                 assign_placeholder = tf.placeholder(tf_dtype,\n\u001b[1;32m   2187\u001b[0m                                                     shape=value.shape)\n\u001b[0;32m-> 2188\u001b[0;31m                 \u001b[0massign_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2189\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2190\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/greg/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking)\u001b[0m\n\u001b[1;32m    525\u001b[0m       \u001b[0mthe\u001b[0m \u001b[0massignment\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mcompleted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m     \"\"\"\n\u001b[0;32m--> 527\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstate_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/greg/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m    272\u001b[0m     return gen_state_ops.assign(\n\u001b[1;32m    273\u001b[0m         \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    275\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/greg/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m     41\u001b[0m   result = _op_def_lib.apply_op(\"Assign\", ref=ref, value=value,\n\u001b[1;32m     42\u001b[0m                                 \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                                 use_locking=use_locking, name=name)\n\u001b[0m\u001b[1;32m     44\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/greg/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    765\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    766\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    768\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/greg/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2630\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2631\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2632\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2633\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2634\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/greg/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1909\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/home/greg/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/greg/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    593\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    594\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    596\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/greg/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    657\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension 1 in both shapes must be equal, but are 200 and 100 for 'Assign_10' (op: 'Assign') with input shapes: [128,200], [128,100]."
     ]
    }
   ],
   "source": [
    "autoencoder.load_weights('../models/autoencoder_%dD.h5' % encoding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ = autoencoder.predict(X, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pixel_mse = ((X - X_) ** 2).squeeze()\n",
    "image_mse = pixel_mse.reshape(pixel_mse.shape[0], -1).mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(image_mse, bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reconstruction_loss_sort_idx = image_mse.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_visualization(X, n, loss_idx):\n",
    "    plt.figure(figsize = (15, 15))\n",
    "    X_high_loss = X[loss_idx[-n**2:]]\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            idx = i * n + j\n",
    "            plt.subplot(n, n, idx + 1)\n",
    "            img = X_high_loss[idx].squeeze()\n",
    "            plt.imshow(img, cmap = 'gray')\n",
    "            plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_visualization(X, 20, reconstruction_loss_sort_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
